# Vision‚ÄëTransformers‚ÄëSegment‚ÄëAnything  
**De las Convoluciones a la Atenci√≥n**  

Una exploraci√≥n profunda del cambio de paradigma en Visi√≥n por Computador: desde las cl√°sicas redes convolucionales (CNN) hasta las arquitecturas basadas en Transformers, a trav√©s de tres pilares recientes en este campo:

- **Vision Transformers (ViT)**  
- **Segment Anything Model (SAM)** de Meta  
- **DETR** (DEtection TRansformer) para detecci√≥n de objetos  

---

## üß† Introducci√≥n

Durante casi una d√©cada, las **CNNs** dominaron tareas de clasificaci√≥n, detecci√≥n y segmentaci√≥n visual gracias a su habilidad para extraer patrones locales con filtros convolucionales. Sin embargo, pensadas para captar contexto local, requer√≠an redes muy profundas para modelar relaciones globales.

La llegada del Transformer en NLP en 2017 cambi√≥ el juego: su mecanismo de **auto-atenci√≥n global** permiti√≥ procesar y comprender dependencias a largo plazo en una sola pasada.

Este repositorio muestra c√≥mo esa evoluci√≥n lleg√≥ a visi√≥n: **ViT**, **SAM** y **DETR** reescribieron las reglas del juego.

---

## üìò Contenido del repositorio

- `notebooks/`: cuadernos con ejemplos funcionales para ViT, SAM y DETR.
- `scripts/`: scripts de entrenamiento o inferencia en PyTorch.
- `README.md`: explicaci√≥n t√©cnica y conceptual del enfoque general.

---

## üìö Pilares Tecnol√≥gicos

### Vision Transformers (ViT)

- **Inspiraci√≥n**: Transformers de NLP adaptados a visi√≥n (imagen = secuencia de parches).  
- **Proceso**: divide la imagen en parches (16√ó16 px), los embebe y aplica atenci√≥n global.  
- **Ventajas**:
  - Alcance global desde la primera capa.
  - Escalabilidad con datos masivos.
  - Excelente transferencia de aprendizaje desde Backbones preentrenados.

**Ejemplo**: Fine‚Äëtuning de ViT con PyTorch sobre CIFAR‚Äë10. El notebook `notebooks/vit_finetune.ipynb` muestra c√≥mo cargar, entrenar y guardar un ViT personalizado.

---

### Segment Anything Model (SAM) de Meta

- Modelo universal de segmentaci√≥n capaz de aislar *cualquier objeto* con prompts interactivos (puntos, bounding boxes o texto).
- **Arquitectura**:
  - Codificador de imagen (ViT pretrained)
  - Codificador de prompts
  - Decodificador de m√°scaras din√°micas
- **Capacitado con m√°s de mil millones de m√°scaras generadas autom√°ticamente en 11M im√°genes**
- **Usos**: edici√≥n visual, rob√≥tica, medicina, etiquetado semiautom√°tico y m√°s.

Ejemplo en `notebooks/sam_interactive.ipynb`: muestra c√≥mo cargar SAM, definir un punto de input y obtener m√∫ltiples m√°scaras con score.

---

### DETR ‚Äì Detecci√≥n de Objetos con Transformers

- Replantea la detecci√≥n: **modelo end‚Äëto‚Äëend sin anchor boxes ni NMS manual**.
- Combina un backbone CNN con varias queries de Transformer.
- Utiliza bipartite matching para asignar predicciones a ground truth.
- Simplifica pipeline y reduce hiperpar√°metros.

Ejemplo en `notebooks/detr_detection.ipynb`: detecci√≥n visual con bounding boxes sobre im√°genes propias, usando modelos preentrenados en COCO.

---

## üîç Comparativa: CNNs vs Transformers en visi√≥n

| Aspecto                  | CNNs                                 | Transformers (ViT / DETR / SAM)       |
|--------------------------|---------------------------------------|---------------------------------------|
| Procesamiento            | Local (kernels)                      | Global (auto-atenci√≥n)                |
| Contexto visual          | Limitado a receptive field local      | Aborda relaciones a gran escala       |
| Interpretabilidad        | Filtros y pooling                    | Atenci√≥n explicable entre parches     |
| Pipeline (detecci√≥n)     | Anchor‚Äëboxes, NMS                    | Predicciones directas con queries     |
| Segmentaci√≥n             | Por clase                             | Interactiva, independiente de clase   |
| Escalabilidad            | Costosa en profundidad                | Escalable con m√°s datos y heads       |

---

## üöÄ Requisitos y uso

1. Clona el repositorio:
```
   git clone https://github.com/Orliluq/Vision-Transformers-Segment-Anything.git
   cd Vision-Transformers-Segment-Anything
```

2. Crea un entorno virtual (recomendado):
```
  python -m venv venv
  source venv/bin/activate
```

3. Instala dependencias:
```
pip install torch torchvision timm segment-anything opencv-python matplotlib
```

4. Explora los notebooks o scripts:

- `notebooks/vit_finetune.ipynb`
- `notebooks/sam_interactive.ipynb`
- `notebooks/detr_detection.ipynb`

5. Cada notebook explica c√≥mo subir im√°genes, cargar modelos, hacer inferencia y visualizar resultados.

---

## ‚ú® Conclusi√≥n
Este repositorio muestra c√≥mo la visi√≥n por computadora est√° dando el gran salto desde la detecci√≥n local con filtros hasta una comprensi√≥n global basada en atenci√≥n:
- **ViT** rompe con el paradigma local al tratar im√°genes como texto (secuencia de parches).
- **SAM** democratiza la segmentaci√≥n interactiva sin clases predeterminadas.
- **DETR** elimina pasos manuales en detecci√≥n, simplificando el dise√±o.

Este es solo el inicio de una nueva era: modelos h√≠bridos que combinen las fortalezas de los CNNs y los Transformers est√°n abriendo un futuro m√°s flexible, eficiente y humano en Visi√≥n por Computador.

---

## üìå Cr√©ditos
- Autora: Orli
- C√≥digo basado en papers de Google Research (ViT), Meta AI (SAM y DETR)
- Ejemplos de implementaci√≥n con frameworks timm, transformers y torchvision.

### ¬°Gracias por explorar esta evoluci√≥n conmigo! üåü
